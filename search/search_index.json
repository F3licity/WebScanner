{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"WebScanner \u00b6 A simple webcrawler to detect broken links in websites recursively using Python. How to use \u00b6 You can use and extend the tool to easily crawl a website and check for broken links and other errors. Download the pageCrawler.py and call it from the terminal using: python3 pageCrawler.py URL_TO_CRAWL Check out all the options using python3 pageCrawler.py --help Run using Docker \u00b6 Use the one-liner below to immediately use the webscanner from DockerHub: docker run --rm emeraldit/webscanner:1.0.0 URL_TO_CRAWL You can also easily run the script using Docker. Build the image: docker build --tag webscanner:1.0.0 . Run the image docker run --rm \\ --name webscanner.container \\ webscanner:1.0.0 \\ --help or on powershell: docker run --rm ` --name webscanner.container ` webscanner:1.0.0 ` --help Use in your own project \u00b6 You can also use the class directly in your own python code: 1 2 3 4 5 6 7 8 9 10 from webscanner import WebScanner crawler = WebScanner ( URL_TO_CRAWL , prefix = OPTIONAL_PREFIX , max_depth = 2 , test_external_urls = True , verbose = 2 , ) crawler . crawl () The prefix determines which links are crawled, for example you can limit the crawling to a specific subdirectory of a domain such as https://domain.com/dir/ Everything above the /dir/ will be ignored, when setting the prefix to this url. The max_depth determines how deep the crawler goes, for example if set to 1, only the links of the initial page are followed and the process stops. If set to 2, all links of the initial page and all following links from the pages that follow the initial page are crawled. Further development \u00b6 Clone this repository git@github.com:F3licity/WebScanner.git Start a new virtual environment on the root folder of this project, using Python 3.8, and activate it. pip3.8 install virtualenv virtualenv venv38 source venv38/bin/activate Install pip and pip-tools . Install the pip libraries required: pip install -r requirements.txt Make sure to check out the CONTRIBUTING.md about the house rules. Start developing! Documentation \u00b6 To contribute please also update the documentation. You can download the required packages from docs-requirements.txt ( pip install -r docs-requirements.txt ). Install the documentation requirements: pip install -r docs-requirements.txt Build the documentation: gendocs --config mkgendocs.yml","title":"WebScanner"},{"location":"index.html#webscanner","text":"A simple webcrawler to detect broken links in websites recursively using Python.","title":"WebScanner"},{"location":"index.html#how-to-use","text":"You can use and extend the tool to easily crawl a website and check for broken links and other errors. Download the pageCrawler.py and call it from the terminal using: python3 pageCrawler.py URL_TO_CRAWL Check out all the options using python3 pageCrawler.py --help","title":"How to use"},{"location":"index.html#run-using-docker","text":"Use the one-liner below to immediately use the webscanner from DockerHub: docker run --rm emeraldit/webscanner:1.0.0 URL_TO_CRAWL You can also easily run the script using Docker. Build the image: docker build --tag webscanner:1.0.0 . Run the image docker run --rm \\ --name webscanner.container \\ webscanner:1.0.0 \\ --help or on powershell: docker run --rm ` --name webscanner.container ` webscanner:1.0.0 ` --help","title":"Run using Docker"},{"location":"index.html#use-in-your-own-project","text":"You can also use the class directly in your own python code: 1 2 3 4 5 6 7 8 9 10 from webscanner import WebScanner crawler = WebScanner ( URL_TO_CRAWL , prefix = OPTIONAL_PREFIX , max_depth = 2 , test_external_urls = True , verbose = 2 , ) crawler . crawl () The prefix determines which links are crawled, for example you can limit the crawling to a specific subdirectory of a domain such as https://domain.com/dir/ Everything above the /dir/ will be ignored, when setting the prefix to this url. The max_depth determines how deep the crawler goes, for example if set to 1, only the links of the initial page are followed and the process stops. If set to 2, all links of the initial page and all following links from the pages that follow the initial page are crawled.","title":"Use in your own project"},{"location":"index.html#further-development","text":"Clone this repository git@github.com:F3licity/WebScanner.git Start a new virtual environment on the root folder of this project, using Python 3.8, and activate it. pip3.8 install virtualenv virtualenv venv38 source venv38/bin/activate Install pip and pip-tools . Install the pip libraries required: pip install -r requirements.txt Make sure to check out the CONTRIBUTING.md about the house rules. Start developing!","title":"Further development"},{"location":"index.html#documentation","text":"To contribute please also update the documentation. You can download the required packages from docs-requirements.txt ( pip install -r docs-requirements.txt ). Install the documentation requirements: pip install -r docs-requirements.txt Build the documentation: gendocs --config mkgendocs.yml","title":"Documentation"},{"location":"API.html","text":"\u00b6 WebScanner \u00b6 source WebScanner ( url , prefix = None , max_depth = None , test_external_urls = False , headers = None , verbose = 0 ) WebCrawler class to automatically find broken links and other issues with a website. Args url (string) : The url to start crawling. prefix (string, optional) : Everything with a different prefix is treated as external. This allows for limiting the crawler to a subdirectory. Defaults to None. max_depth (integer, optional) : Maximum crawl depth, None is unlimited. Defaults to None. test_external_urls (bool, optional) : Test the status code of external links or not. Defaults to False. headers (dict, optional) : Custom headers dictionary for example to provide login details. Defaults to None. verbose (int, optional) : How much we output, should be 0,1 or 2. At level 0 only broken links are reported. Defaults to 0. Methods: .crawl \u00b6 source . crawl ( current_link = None , depth = 0 ) Crawl through the documentation page and return the status code of the links. Keep track of visited links. When a new link is visited request its HTML and get all anchors found in it. If they are external links, return their status code. If they are internal and they are relative paths, make them first complete and then return their status. For every link visited go deeper and deeper to visit all links found in it. Args current_link (string, optional) : The current link to download and crawl (if not visited yet). Defaults to None. depth (int, optional) : The current recursive depth. Defaults to 0. .clean_url \u00b6 source . clean_url ( url ) Remove the query and anchors from a url. Args url (string) : Url to clean. Returns string : url without anchors and query parameters. .is_external \u00b6 source . is_external ( url ) Check if url is in the part we want to crawl. Args url (string) : The url to check. Returns bool : boolean indicating whether we treat the url as external or not.","title":"API"},{"location":"API.html#_1","text":"","title":""},{"location":"API.html#webscanner","text":"source WebScanner ( url , prefix = None , max_depth = None , test_external_urls = False , headers = None , verbose = 0 ) WebCrawler class to automatically find broken links and other issues with a website. Args url (string) : The url to start crawling. prefix (string, optional) : Everything with a different prefix is treated as external. This allows for limiting the crawler to a subdirectory. Defaults to None. max_depth (integer, optional) : Maximum crawl depth, None is unlimited. Defaults to None. test_external_urls (bool, optional) : Test the status code of external links or not. Defaults to False. headers (dict, optional) : Custom headers dictionary for example to provide login details. Defaults to None. verbose (int, optional) : How much we output, should be 0,1 or 2. At level 0 only broken links are reported. Defaults to 0. Methods:","title":"WebScanner"},{"location":"API.html#crawl","text":"source . crawl ( current_link = None , depth = 0 ) Crawl through the documentation page and return the status code of the links. Keep track of visited links. When a new link is visited request its HTML and get all anchors found in it. If they are external links, return their status code. If they are internal and they are relative paths, make them first complete and then return their status. For every link visited go deeper and deeper to visit all links found in it. Args current_link (string, optional) : The current link to download and crawl (if not visited yet). Defaults to None. depth (int, optional) : The current recursive depth. Defaults to 0.","title":".crawl"},{"location":"API.html#clean_url","text":"source . clean_url ( url ) Remove the query and anchors from a url. Args url (string) : Url to clean. Returns string : url without anchors and query parameters.","title":".clean_url"},{"location":"API.html#is_external","text":"source . is_external ( url ) Check if url is in the part we want to crawl. Args url (string) : The url to check. Returns bool : boolean indicating whether we treat the url as external or not.","title":".is_external"},{"location":"contributing.html","text":"Contributing \u00b6 File an issue to notify the maintainers about what you're working on. Fork the repo, develop and test your code changes, add docs. Make sure that your commit messages clearly describe the changes. Send a pull request. File an Issue \u00b6 Use the issue tracker to start the discussion. It is possible that someone else is already working on your idea, that your approach is not quite right, or that the functionality exists already. The ticket you file in the issue tracker will be used to hash that all out. Style Guides \u00b6 Write in UTF-8 in Python 3 Use modular architecture to group similar functions, classes, etc. Always use 4 spaces for indentation (don't use tabs) Try to limit line length to 88 characters Class names should always be capitalized Function names should always be lowercase Look at the existing style and adhere accordingly Fork the Repository \u00b6 Be sure to add the relevant tests before raising a pull request. You should also build the docs yourself and make sure they're readable. Make the Pull Request \u00b6 Once you have made all your changes, tests, and updated the documentation, make a pull request to move everything back into the main branch of the repository . Be sure to reference the original issue in the pull request. Expect some back-and-forth in regard to style and compliance of these rules.","title":"Contributing"},{"location":"contributing.html#contributing","text":"File an issue to notify the maintainers about what you're working on. Fork the repo, develop and test your code changes, add docs. Make sure that your commit messages clearly describe the changes. Send a pull request.","title":"Contributing"},{"location":"contributing.html#file-an-issue","text":"Use the issue tracker to start the discussion. It is possible that someone else is already working on your idea, that your approach is not quite right, or that the functionality exists already. The ticket you file in the issue tracker will be used to hash that all out.","title":"File an Issue"},{"location":"contributing.html#style-guides","text":"Write in UTF-8 in Python 3 Use modular architecture to group similar functions, classes, etc. Always use 4 spaces for indentation (don't use tabs) Try to limit line length to 88 characters Class names should always be capitalized Function names should always be lowercase Look at the existing style and adhere accordingly","title":"Style Guides"},{"location":"contributing.html#fork-the-repository","text":"Be sure to add the relevant tests before raising a pull request. You should also build the docs yourself and make sure they're readable.","title":"Fork the Repository"},{"location":"contributing.html#make-the-pull-request","text":"Once you have made all your changes, tests, and updated the documentation, make a pull request to move everything back into the main branch of the repository . Be sure to reference the original issue in the pull request. Expect some back-and-forth in regard to style and compliance of these rules.","title":"Make the Pull Request"}]}